{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/testdoc.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.types.doc import BoundingBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import torch\n",
    "import os\n",
    "import fitz\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling_core.types.doc import PictureItem, ImageRefMode\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption,  ImageFormatOption\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    EasyOcrOptions,\n",
    "    TesseractOcrOptions,\n",
    "    AcceleratorDevice,\n",
    "    AcceleratorOptions,\n",
    "    PdfPipelineOptions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pipeline_options(**kwargs):\n",
    "        pipeline_options = PdfPipelineOptions()\n",
    "        pipeline_options.do_ocr = kwargs.get('do_ocr', True)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            pipeline_options.ocr_options = EasyOcrOptions(\n",
    "                use_gpu=kwargs.get('use_gpu', True),\n",
    "                lang=kwargs.get('lang', ['en', 'ne']),\n",
    "                confidence_threshold=kwargs.get('confidence_threshold', 0.1),\n",
    "            )\n",
    "            pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "                num_threads=4, device=AcceleratorDevice.CUDA\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            pipeline_options.ocr_options = TesseractOcrOptions(\n",
    "                lang=kwargs.get('tess_lang', ['eng', 'nep']),\n",
    "            )\n",
    "                \n",
    "        pipeline_options.do_table_structure = kwargs.get(\"do_table_structure\", True)\n",
    "        pipeline_options.table_structure_options.do_cell_matching = kwargs.get(\"do_cell_matching\", True)\n",
    "        # pipeline_options.images_scale = kwargs.get(\"images_scale\", 2.0)\n",
    "        # pipeline_options.generate_page_images = kwargs.get(\"generate_page_images\", True)\n",
    "        # pipeline_options.generate_picture_images = kwargs.get(\"generate_picture_images\", True)\n",
    "\n",
    "        return pipeline_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_options = _get_pipeline_options(do_ocr=False)\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "            backend=PyPdfiumDocumentBackend,\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_result = doc_converter.convert(path).document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_items_by_bbox_and_page(docling_document, target_bbox, target_page_no):\n",
    "    \"\"\"\n",
    "    Extracts text and table items from a docling document within a given bounding box and page number.\n",
    "\n",
    "    Args:\n",
    "        docling_document (object): The document object obtained from `doc_converter.convert(path).document`.\n",
    "        target_bbox (tuple): The target bounding box (x0, y0, x1, y1).\n",
    "        target_page_no (int): The page number to filter items.\n",
    "\n",
    "    Returns:\n",
    "        list: Combined list of text and table objects matching the criteria.\n",
    "    \"\"\"\n",
    "    x0, y0, x1, y1 = target_bbox\n",
    "    items = []\n",
    "\n",
    "    # Combine texts and tables\n",
    "    elements = docling_document.texts + docling_document.tables\n",
    "\n",
    "    for element in elements:\n",
    "        # Get the page number and bbox\n",
    "        prov = element.prov[0]\n",
    "        if prov.page_no != target_page_no:\n",
    "            continue  # Skip if the page number doesn't match\n",
    "\n",
    "        element_bbox = prov.bbox.as_tuple()\n",
    "        ex0, ey0, ex1, ey1 = element_bbox\n",
    "\n",
    "        # Check if the element bbox is inside the target bbox\n",
    "        if ex0 >= x0 and ey0 >= y0 and ex1 <= x1 and ey1 <= y1:\n",
    "            items.append(element)\n",
    "\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.types.doc import BoundingBox, CoordOrigin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "box = BoundingBox.from_tuple((56.79999923706055, 236.43328857421875, 559.0999755859375, 720.9435424804688), origin=CoordOrigin.TOPLEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, y0, x1, y1 = box.as_tuple()\n",
    "items = []\n",
    "\n",
    "# Combine texts and tables\n",
    "elements = conv_result.texts + conv_result.tables\n",
    "\n",
    "for element in elements:\n",
    "    # Get the page number and bbox\n",
    "    prov = element.prov[0]\n",
    "    if prov.page_no != 4:\n",
    "        continue  # Skip if the page number doesn't match\n",
    "\n",
    "    element_bbox = prov.bbox.as_tuple()\n",
    "    ex0, ey0, ex1, ey1 = element_bbox\n",
    "\n",
    "    # Check if the element bbox is inside the target bbox\n",
    "    if ex0 >= x0 and ey0 >= y0 and ex1 <= x1 and ey1 <= y1:\n",
    "        items.append(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elemet_refs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_result.body.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_element_index(cref):\n",
    "    children_refs = [c.cref for c in conv_result.body.children]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_refs = [c.cref for c in conv_result.body.children] # But some refs have #/groups/ reference, that contain multiple children elements\n",
    "def resolve_children(ref, conv_result):\n",
    "    \"\"\"\n",
    "    Resolves a reference, replacing group references with their children recursively.\n",
    "    \"\"\"\n",
    "    if ref.startswith(\"#/groups/\"):\n",
    "        group_index = int(ref.split(\"/\")[-1])\n",
    "        group_children = [resolve_children(c.cref, conv_result) for c in conv_result.groups[group_index].children]\n",
    "        return [child for sublist in group_children for child in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    else:\n",
    "        return ref\n",
    "    \n",
    "updated_children_refs = []\n",
    "for ref in children_refs:\n",
    "    resolved = resolve_children(ref, conv_result)\n",
    "    if isinstance(resolved, list):\n",
    "        updated_children_refs.extend(resolved)\n",
    "    else:\n",
    "        updated_children_refs.append(resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_result.texts[6].get_ref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(conv_result.export_to_markdown(from_element=0, to_element=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_ref = conv_result.texts[5].get_ref().cref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_refs.index(elem_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import npttf2utf\n",
    "\n",
    "def is_english_word(word: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a word is an English word.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the word is an English word, False otherwise.\n",
    "    \"\"\"\n",
    "    english_dict = enchant.Dict(\"en_US\")\n",
    "    try:\n",
    "        word = word.lower().strip()\n",
    "        word = word.strip(string.punctuation)\n",
    "        if not word.isalpha():\n",
    "            return False\n",
    "        return english_dict.check(word)\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def map_to_unicode(text, check_english_words: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Map the text to Unicode characters using the font mapper.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to map.\n",
    "        check_english_words (bool): Whether to check and skip English words (default: False).\n",
    "\n",
    "    Returns:\n",
    "        str: The text mapped to Unicode characters.\n",
    "    \"\"\"\n",
    "    mapper = npttf2utf.FontMapper(\n",
    "        os.path.abspath(\n",
    "            os.path.join(\n",
    "                \"..\",\n",
    "                \"assets\", \n",
    "                \"font_mapper.json\"\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    if not check_english_words:\n",
    "        return mapper.map_to_unicode(\n",
    "            text, \n",
    "            unescape_html_input=False, \n",
    "            escape_html_output=False\n",
    "        )\n",
    "    \n",
    "    mapped_text = []\n",
    "    for word in text.split(\" \"):\n",
    "        if not is_english_word(word):\n",
    "            mapped_word = mapper.map_to_unicode(\n",
    "                word, \n",
    "                unescape_html_input=False, \n",
    "                escape_html_output=False\n",
    "            )\n",
    "            mapped_text.append(mapped_word)\n",
    "        else:\n",
    "            mapped_text.append(word)\n",
    "\n",
    "    return \" \".join(mapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_in_bbox(doc: fitz.Document, page: int, bbox: fitz.Rect) -> str:\n",
    "    \"\"\"\n",
    "    Extract text within a bounding box on a given page and map to Unicode.\n",
    "    \"\"\"\n",
    "    # Extract text and associated font details from the bounding box\n",
    "    page_obj = doc[page]\n",
    "    text_instances = page_obj.get_text(\"dict\", clip=bbox)[\"blocks\"]\n",
    "    fonts_to_map = []\n",
    "\n",
    "    fonts_file_path = os.path.abspath(\n",
    "    os.path.join(\n",
    "        \"..\",\n",
    "        \"assets\", \n",
    "        \"nepali_fonts.txt\"\n",
    "    ))\n",
    "\n",
    "    with open(fonts_file_path, \"r\") as f:\n",
    "        fonts_to_map = f.read().split(\"\\n\")\n",
    "    extracted_text = []\n",
    "    for block in text_instances:\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                font = span.get(\"font\", \"\")\n",
    "                text = span.get(\"text\", \"\")\n",
    "                if font in fonts_to_map:\n",
    "                    text = map_to_unicode(text)  # Convert to Unicode if the font is Preeti\n",
    "                extracted_text.append(text)\n",
    "\n",
    "    return \" \".join(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitz_doc=fitz.open(path)\n",
    "for item in items:\n",
    "    prov = item.prov[0]\n",
    "    page_no = prov.page_no\n",
    "\n",
    "    bbox = prov.bbox\n",
    "    bbox = bbox.to_top_left_origin(conv_result.pages[page_no].size.height) \n",
    "\n",
    "    fitz_text = get_text_in_bbox(fitz_doc, page=page_no-1, bbox=bbox.as_tuple())\n",
    "    if len(fitz_text)>10:\n",
    "        item.text = fitz_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = conv_result.tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.prov[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in table.data.table_cells:\n",
    "    page_no = table.prov[0].page_no\n",
    "    page_height = conv_result.pages[page_no].size.height\n",
    "    box = cell.bbox.to_top_left_origin(page_height)\n",
    "\n",
    "    cell.text = get_text_in_bbox(fitz_doc, page_no-1, box.as_tuple())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(table.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(conv_result.export_to_markdown(from_element=20, to_element=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
